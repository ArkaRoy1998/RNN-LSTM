import numpy as np
import os
import json
import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, LSTM, TimeDistributed, GlobalAveragePooling2D
from tensorflow.keras.applications import ResNet50
from scipy.spatial.transform import Rotation as R
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import time

# Base paths and parameters
dataset_path = '/content/sample_data/speed'  # Replace with the actual path to the SPED dataset
image_size = (224, 224)  # Image size for ResNet50
sequence_length = 10  # Length of image sequences
output_size = 3  # Number of angular velocities to predict (yaw, pitch, roll)
angular_velocity_save_path = '/content/sample_data/angular_velocities.json'  # Path to save angular velocities

# Function to get the labels path based on the dataset type
def get_labels_path(dataset_type):
    if dataset_type == 'real':
        return os.path.join(dataset_path, 'real.json')
    elif dataset_type == 'real_test':
        return os.path.join(dataset_path, 'real_test.json')
    elif dataset_type == 'test':
        return os.path.join(dataset_path, 'test.json')
    elif dataset_type == 'train':
        return os.path.join(dataset_path, 'train.json')
    else:
        raise ValueError("Invalid dataset type. Choose from 'real', 'real_test', 'test', or 'train'.")

# Example usage: Set labels_path for 'train' dataset
dataset_type = 'train'  # Choose the dataset you want to work with: 'real', 'real_test', 'test', 'train'
labels_path = get_labels_path(dataset_type)

print("Labels Path:", labels_path)

# Function to load and preprocess images for ResNet50
IMAGE_SIZE = (224, 224)
AUTOTUNE = tf.data.AUTOTUNE

@tf.function
def get_image(filenames):
    image = tf.io.read_file(filenames)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, IMAGE_SIZE)
    image = tf.keras.applications.resnet50.preprocess_input(image)
    return image

# Function to create sequences of data
def in_sequence(data, sequence_length):
    data = data.window(sequence_length, shift=1, drop_remainder=True)
    data = data.flat_map(lambda window: window.batch(sequence_length))
    return data

# Data loader function to create a TensorFlow dataset
def dataloader(labels, sequence_length=5, batch_size=64, type_data="train"):
    tensors = {
        "filename": tf.convert_to_tensor([f"/content/sample_data/speed/images/{type_data}/{d['filename']}" for d in labels]),
        "q_vbs2tango": tf.convert_to_tensor([d["q_vbs2tango"] for d in labels]),
        "r_Vo2To_vbs_true": tf.convert_to_tensor([d["r_Vo2To_vbs_true"] for d in labels]),
    }

    files = tf.data.Dataset.from_tensor_slices(tensors['filename'])
    files = files.map(get_image, num_parallel_calls=AUTOTUNE)
    quaternions = tf.data.Dataset.from_tensor_slices(tensors['q_vbs2tango'])
    positions = tf.data.Dataset.from_tensor_slices(tensors['r_Vo2To_vbs_true'])
    
    files = in_sequence(files, sequence_length)
    quaternions = in_sequence(quaternions, sequence_length)
    positions = in_sequence(positions, sequence_length)
    data = tf.data.Dataset.zip((files, quaternions, positions))
    data = data.batch(batch_size)
    data = data.prefetch(AUTOTUNE)
    return data

# Load dataset labels
with open(labels_path, 'r') as f:
    labels_data = json.load(f)

# Print the first item to understand its layout
print(json.dumps(labels_data[0], indent=4))

# Define a function to compute angular velocities from quaternions
def compute_angular_velocity(quaternions, time_stamps):
    angular_velocities = []
    for i in range(1, len(quaternions)):
        q1 = R.from_quat(quaternions[i - 1])
        q2 = R.from_quat(quaternions[i])
        delta_t = time_stamps[i] - time_stamps[i - 1]

        delta_q = q2 * q1.inv()
        angle = np.linalg.norm(delta_q.as_rotvec())
        axis = delta_q.as_rotvec() / angle if angle != 0 else [0, 0, 0]
        
        angular_velocity = (angle / delta_t) * np.array(axis)
        angular_velocities.append(angular_velocity)

    return np.array(angular_velocities)

# Function to define and compile the model
def create_model():
    resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(*image_size, 3))
    resnet_model = Sequential([
        resnet_base,
        GlobalAveragePooling2D()
    ])
    time_distributed_resnet = TimeDistributed(resnet_model)
    
    model = Sequential()
    model.add(time_distributed_resnet)
    model.add(LSTM(256, return_sequences=True))
    model.add(TimeDistributed(Dense(output_size)))
    
    # Compile the model
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# Function to train and evaluate the model on a single train/validation split
def train_and_evaluate(train_labels, val_labels, fold_index):
    # Create data loaders for training and validation sets
    train_dataset = dataloader(train_labels, sequence_length=sequence_length, batch_size=64, type_data=dataset_type)
    val_dataset = dataloader(val_labels, sequence_length=sequence_length, batch_size=64, type_data=dataset_type)
    
    # Define and compile the model
    model = create_model()
    
    # Training the model
    all_angular_velocities = []  # Reset for each fold

    for image_batch, quat_batch, pos_batch in train_dataset:
        angular_velocity_sequences = []
        timestamps = np.arange(sequence_length) * 0.1  # Dummy timestamps for each sequence
        
        for quat_seq in quat_batch:
            ang_vel = compute_angular_velocity(quat_seq.numpy(), timestamps)
            padded_ang_vel = np.vstack(([0, 0, 0], ang_vel))
            angular_velocity_sequences.append(padded_ang_vel)
        
        angular_velocity_sequences = np.array(angular_velocity_sequences)
        model.fit(image_batch, angular_velocity_sequences, epochs=1, batch_size=64, verbose=1)

    # Save the model for this fold
    model_path = f'pose_estimation_with_angular_velocity_fold_{fold_index}.h5'
    model.save(model_path)
    print(f"Model saved to {model_path}")
    
    # Evaluate the model on the validation set
    all_predicted_angular_velocities = []
    all_true_angular_velocities = []

    for val_image_batch, val_quat_batch, val_pos_batch in val_dataset:
        true_angular_velocity_sequences = []
        timestamps = np.arange(sequence_length) * 0.1
        
        for quat_seq in val_quat_batch:
            true_ang_vel = compute_angular_velocity(quat_seq.numpy(), timestamps)
            padded_true_ang_vel = np.vstack(([0, 0, 0], true_ang_vel))
            true_angular_velocity_sequences.append(padded_true_ang_vel)
        
        true_angular_velocity_sequences = np.array(true_angular_velocity_sequences)
        
        # Predict using the model
        predicted_angular_velocities = model.predict(val_image_batch)
        
           # Convert lists to numpy arrays for evaluation
    all_predicted_angular_velocities = np.array(all_predicted_angular_velocities)
    all_true_angular_velocities = np.array(all_true_angular_velocities)
    
    # Compute evaluation metrics
    mae = mean_absolute_error(
        all_true_angular_velocities.reshape(-1, 3),
        all_predicted_angular_velocities.reshape(-1, 3)
    )

    print(f"Mean Absolute Error on validation set for fold {fold_index}: {mae}")
    return mae

# Perform the manual 5-fold cross-validation using 80:20 splits
num_folds = 5
mae_scores = []

# Shuffle and split the data into 5 different training and validation sets
for fold in range(num_folds):
    print(f"Starting fold {fold+1}/{num_folds}")

    # Shuffle the data indices
    np.random.shuffle(indices)

    # Split the data into 80% training and 20% validation
    train_indices, val_indices = train_test_split(indices, test_size=0.2, random_state=fold)

    # Create train and validation label sets
    train_labels = [labels_data[i] for i in train_indices]
    val_labels = [labels_data[i] for i in val_indices]

    # Train and evaluate the model on this split
    mae = train_and_evaluate(train_labels, val_labels, fold + 1)
    mae_scores.append(mae)

# Compute and print the average MAE across all folds
average_mae = np.mean(mae_scores)
print(f"Average Mean Absolute Error across all folds: {average_mae}")

# Evaluate ensemble on test data
# Paths to the test dataset and labels
test_dataset_path = '/content/sample_data/speed/images/test'  # Replace with your test dataset path
test_labels_path = '/content/sample_data/speed/test.json'  # Replace with your test labels JSON file path

# Load test dataset labels
with open(test_labels_path, 'r') as f:
    test_labels_data = json.load(f)

# Create the test data generator
test_data_generator = dataloader(test_labels_data, sequence_length=sequence_length, batch_size=64, type_data='test')

all_predicted_angular_velocities = []
all_true_angular_velocities = []

# Generate predictions using ensemble of models
for batch_images, batch_quaternions in test_data_generator:
    batch_predictions = []

    # Generate predictions for each model in the ensemble
    for fold in range(num_folds):
        model_path = f'pose_estimation_with_angular_velocity_fold_{fold+1}.h5'
        model = load_model(model_path)
        predicted_angular_velocities = model.predict(batch_images)
        batch_predictions.append(predicted_angular_velocities)
    
    # Average the predictions
    avg_predictions = np.mean(batch_predictions, axis=0)
    
    all_predicted_angular_velocities.extend(avg_predictions.tolist())
    
    # Compute true angular velocities for comparison
    true_angular_velocity_sequences = []
    timestamps = np.arange(sequence_length) * 0.1
    
    for quat_seq in batch_quaternions:
        true_ang_vel = compute_angular_velocity(quat_seq.numpy(), timestamps)
        padded_true_ang_vel = np.vstack(([0, 0, 0], true_ang_vel))
        true_angular_velocity_sequences.append(padded_true_ang_vel)
    
    all_true_angular_velocities.extend(true_angular_velocity_sequences)

# Convert lists to numpy arrays for evaluation
all_predicted_angular_velocities = np.array(all_predicted_angular_velocities)
all_true_angular_velocities = np.array(all_true_angular_velocities)

# Compute evaluation metrics
mae = mean_absolute_error(
    all_true_angular_velocities.reshape(-1, 3),
    all_predicted_angular_velocities.reshape(-1, 3)
)

print(f"Mean Absolute Error on Test Data with Ensembling: {mae}")

# Optional: Compare a few predicted vs true angular velocities
for i in range(min(5, len(all_predicted_angular_velocities))):
    print(f"Sequence {i+1}")
    print("True Angular Velocities:\n", all_true_angular_velocities[i])
    print("Predicted Angular Velocities:\n", all_predicted_angular_velocities[i])
    print("\n")

    #
